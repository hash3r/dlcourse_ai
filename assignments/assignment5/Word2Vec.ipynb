{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.1 - Word2Vec\n",
    "\n",
    "В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n",
    "Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "refugees ['hear', 'afghan', 'unaffected']\n",
      "leading ['weakened', 'miscast']\n",
      "send ['something', 'happens', 'direction']\n",
      "diapers ['infantile', 'changing']\n",
      "faith ['offended', 'lack']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {}\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for word in words:\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1 \n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper:\n",
    "        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "        \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имплеменируем PyTorch-style Dataset для Word2Vec\n",
    "\n",
    "Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n",
    "\n",
    "Например:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `love`\n",
    "- input: `orders`, target: `nicest`\n",
    "- input: `orders`, target: `to`\n",
    "- input: `orders`, target: `50-year`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.]), target: 130\n"
     ]
    }
   ],
   "source": [
    "class Word2VecPlain(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for plain Word2Vec.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_contexts=30000):\n",
    "        '''\n",
    "        Initializes Word2VecPlain, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instace\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.num_contexts = num_contexts\n",
    "        self.samples = []\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        self.samples = []\n",
    "        for x in range(0, self.num_contexts):\n",
    "            center_word, other_words = self.data.get_random_context(5)\n",
    "            for word in other_words:\n",
    "                self.samples.append((center_word, word))\n",
    "\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of sample\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - torch.Tensor with one-hot representation of the input vector\n",
    "        output_index - index of the target word (not torch.Tensor!)\n",
    "        '''\n",
    "        # TODO: Generate tuple of 2 return arguments for i-th sample    \n",
    "        input_word, output_word = self.samples[index]\n",
    "        vocabulary_size = data.num_tokens()\n",
    "        \n",
    "        input_vector = torch.zeros(vocabulary_size).float()\n",
    "        input_vector[self.data.index_by_token[input_word]] = 1.0\n",
    "        \n",
    "        output_index = self.data.index_by_token[output_word]\n",
    "        \n",
    "#         output_vector = torch.zeros(vocabulary_size).long()\n",
    "#         output_vector[self.data.index_by_token[output_word]] = 1.0\n",
    "        \n",
    "        return input_vector, output_index\n",
    "\n",
    "dataset = Word2VecPlain(data, 10)\n",
    "dataset.generate_dataset()\n",
    "input_vector, target = dataset[3]\n",
    "print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n",
    "assert isinstance(input_vector, torch.Tensor)\n",
    "assert torch.sum(input_vector) == 1.0\n",
    "assert input_vector.shape[0] == data.num_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель и тренируем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=19538, out_features=5, bias=False)\n",
       "  (1): Linear(in_features=5, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecPlain(data, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "# We'll be training very small word vectors!\n",
    "wordvec_dim = 5\n",
    "\n",
    "# We can use a standard sequential model for this\n",
    "nn_model = nn.Sequential(\n",
    "            nn.Linear(data.num_tokens(), wordvec_dim, bias=False),\n",
    "            nn.Linear(wordvec_dim, data.num_tokens(), bias=False), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 19538]) (19538, 5)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d55e9ea77d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muntrained_output_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0muntrained_input_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#???bug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0muntrained_output_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos \n",
    "    # Hint: you can access weights as Tensors through nn.Linear class attributes\n",
    "    \n",
    "    return nn_model[0].weight.data.t().clone(), nn_model[1].weight.data.t().clone()\n",
    "\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "print(untrained_output_vectors.shape, (data.num_tokens(), wordvec_dim))\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim) #???bug\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
    "    '''\n",
    "    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        dataset.generate_dataset() # Regenerate dataset every epoch\n",
    "        \n",
    "        # TODO Implement training for this model\n",
    "        # Note we don't have any validation set here because our purpose is the word vectors,\n",
    "        # not the predictive performance of the model\n",
    "        #\n",
    "        # And don't forget to step the learing rate scheduler!  \n",
    "\n",
    "        # Decay Learning Rate\n",
    "        scheduler.step()\n",
    "        print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "                    \n",
    "#             vocabulary_size = data.num_tokens()\n",
    "#             target_vector = torch.zeros(vocabulary_size).float()\n",
    "#             target_vector[y] = 1.0\n",
    "#             print('Pred', prediction, 'y', y)\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value.item() #prevent memory leak  with item() or float\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        \n",
    "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **8.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [10.0]\n",
      "Epoch 0, Average loss: 9.893092, Train accuracy: 0.000097\n",
      "Epoch: 1 LR: [10.0]\n",
      "Epoch 1, Average loss: 9.887800, Train accuracy: 0.000266\n",
      "Epoch: 2 LR: [10.0]\n",
      "Epoch 2, Average loss: 9.880504, Train accuracy: 0.000906\n",
      "Epoch: 3 LR: [10.0]\n",
      "Epoch 3, Average loss: 9.863289, Train accuracy: 0.001608\n",
      "Epoch: 4 LR: [10.0]\n",
      "Epoch 4, Average loss: 9.811964, Train accuracy: 0.002241\n",
      "Epoch: 5 LR: [10.0]\n",
      "Epoch 5, Average loss: 9.725569, Train accuracy: 0.002396\n",
      "Epoch: 6 LR: [10.0]\n",
      "Epoch 6, Average loss: 9.633036, Train accuracy: 0.002571\n",
      "Epoch: 7 LR: [10.0]\n",
      "Epoch 7, Average loss: 9.552399, Train accuracy: 0.002638\n",
      "Epoch: 8 LR: [10.0]\n",
      "Epoch 8, Average loss: 9.484607, Train accuracy: 0.003155\n",
      "Epoch: 9 LR: [10.0]\n",
      "Epoch 9, Average loss: 9.436129, Train accuracy: 0.002288\n",
      "Epoch: 10 LR: [10.0]\n",
      "Epoch 10, Average loss: 9.398449, Train accuracy: 0.002983\n",
      "Epoch: 11 LR: [10.0]\n",
      "Epoch 11, Average loss: 9.361136, Train accuracy: 0.003011\n",
      "Epoch: 12 LR: [10.0]\n",
      "Epoch 12, Average loss: 9.321132, Train accuracy: 0.003069\n",
      "Epoch: 13 LR: [10.0]\n",
      "Epoch 13, Average loss: 9.295539, Train accuracy: 0.003029\n",
      "Epoch: 14 LR: [10.0]\n",
      "Epoch 14, Average loss: 9.252731, Train accuracy: 0.003392\n",
      "Epoch: 15 LR: [10.0]\n",
      "Epoch 15, Average loss: 9.221153, Train accuracy: 0.003440\n",
      "Epoch: 16 LR: [10.0]\n",
      "Epoch 16, Average loss: 9.195513, Train accuracy: 0.003443\n",
      "Epoch: 17 LR: [10.0]\n",
      "Epoch 17, Average loss: 9.167658, Train accuracy: 0.004068\n",
      "Epoch: 18 LR: [10.0]\n",
      "Epoch 18, Average loss: 9.136532, Train accuracy: 0.004296\n",
      "Epoch: 19 LR: [10.0]\n",
      "Epoch 19, Average loss: 9.115318, Train accuracy: 0.004611\n",
      "Epoch: 20 LR: [10.0]\n",
      "Epoch 20, Average loss: 9.083862, Train accuracy: 0.004947\n",
      "Epoch: 21 LR: [10.0]\n",
      "Epoch 21, Average loss: 9.066183, Train accuracy: 0.005095\n",
      "Epoch: 22 LR: [10.0]\n",
      "Epoch 22, Average loss: 9.037660, Train accuracy: 0.005121\n",
      "Epoch: 23 LR: [10.0]\n",
      "Epoch 23, Average loss: 9.018974, Train accuracy: 0.004797\n",
      "Epoch: 24 LR: [10.0]\n",
      "Epoch 24, Average loss: 9.003190, Train accuracy: 0.004827\n",
      "Epoch: 25 LR: [10.0]\n",
      "Epoch 25, Average loss: 8.981549, Train accuracy: 0.005595\n",
      "Epoch: 26 LR: [10.0]\n",
      "Epoch 26, Average loss: 8.973886, Train accuracy: 0.004566\n",
      "Epoch: 27 LR: [10.0]\n",
      "Epoch 27, Average loss: 8.951272, Train accuracy: 0.005762\n",
      "Epoch: 28 LR: [10.0]\n",
      "Epoch 28, Average loss: 8.929535, Train accuracy: 0.005774\n",
      "Epoch: 29 LR: [10.0]\n",
      "Epoch 29, Average loss: 8.905998, Train accuracy: 0.005993\n",
      "Epoch: 30 LR: [10.0]\n",
      "Epoch 30, Average loss: 8.906331, Train accuracy: 0.005286\n",
      "Epoch: 31 LR: [10.0]\n",
      "Epoch 31, Average loss: 8.892215, Train accuracy: 0.005493\n",
      "Epoch: 32 LR: [10.0]\n",
      "Epoch 32, Average loss: 8.875664, Train accuracy: 0.005645\n",
      "Epoch: 33 LR: [10.0]\n",
      "Epoch 33, Average loss: 8.879015, Train accuracy: 0.006176\n",
      "Epoch: 34 LR: [10.0]\n",
      "Epoch 34, Average loss: 8.851952, Train accuracy: 0.005926\n",
      "Epoch: 35 LR: [10.0]\n",
      "Epoch 35, Average loss: 8.854274, Train accuracy: 0.005963\n",
      "Epoch: 36 LR: [10.0]\n",
      "Epoch 36, Average loss: 8.849873, Train accuracy: 0.005905\n",
      "Epoch: 37 LR: [10.0]\n",
      "Epoch 37, Average loss: 8.840302, Train accuracy: 0.006427\n",
      "Epoch: 38 LR: [10.0]\n",
      "Epoch 38, Average loss: 8.831941, Train accuracy: 0.006135\n",
      "Epoch: 39 LR: [10.0]\n",
      "Epoch 39, Average loss: 8.813400, Train accuracy: 0.005708\n",
      "Epoch: 40 LR: [10.0]\n",
      "Epoch 40, Average loss: 8.805130, Train accuracy: 0.006492\n",
      "Epoch: 41 LR: [10.0]\n",
      "Epoch 41, Average loss: 8.799712, Train accuracy: 0.006225\n",
      "Epoch: 42 LR: [10.0]\n",
      "Epoch 42, Average loss: 8.803055, Train accuracy: 0.006078\n",
      "Epoch: 43 LR: [10.0]\n",
      "Epoch 43, Average loss: 8.784985, Train accuracy: 0.006442\n",
      "Epoch: 44 LR: [10.0]\n",
      "Epoch 44, Average loss: 8.777469, Train accuracy: 0.006243\n",
      "Epoch: 45 LR: [10.0]\n",
      "Epoch 45, Average loss: 8.796622, Train accuracy: 0.005775\n",
      "Epoch: 46 LR: [10.0]\n",
      "Epoch 46, Average loss: 8.787028, Train accuracy: 0.006543\n",
      "Epoch: 47 LR: [10.0]\n",
      "Epoch 47, Average loss: 8.791535, Train accuracy: 0.005825\n",
      "Epoch: 48 LR: [10.0]\n",
      "Epoch 48, Average loss: 8.771207, Train accuracy: 0.006251\n",
      "Epoch: 49 LR: [10.0]\n",
      "Epoch 49, Average loss: 8.769580, Train accuracy: 0.005917\n",
      "Epoch: 50 LR: [1.0]\n",
      "Epoch 50, Average loss: 8.761878, Train accuracy: 0.006853\n",
      "Epoch: 51 LR: [1.0]\n",
      "Epoch 51, Average loss: 8.718521, Train accuracy: 0.007609\n",
      "Epoch: 52 LR: [1.0]\n",
      "Epoch 52, Average loss: 8.705097, Train accuracy: 0.008550\n",
      "Epoch: 53 LR: [1.0]\n",
      "Epoch 53, Average loss: 8.687564, Train accuracy: 0.008764\n",
      "Epoch: 54 LR: [1.0]\n",
      "Epoch 54, Average loss: 8.684008, Train accuracy: 0.008086\n",
      "Epoch: 55 LR: [1.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dc7f415e357c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-36c8536c2d71>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, train_loader, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mcorrect_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finally, let's train the model!\n",
    "\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=10, weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "\n",
    "loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8de6f4804248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize training graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m212\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n",
    "\n",
    "Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.suptitle(title)\n",
    "    for color, words in visualize_words.items():\n",
    "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "        for i, word in enumerate(words):\n",
    "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
